{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing needed packages and Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pkg_resources\n",
    "import json\n",
    "import pickle\n",
    "import psutil\n",
    "\n",
    "# From forecast_utils\n",
    "levels = 2\n",
    "sys.path.append(os.path.abspath(os.path.join(\".\", \"../\"*levels)))\n",
    "import forecast_utils as utils\n",
    "\n",
    "# Prophet Package:\n",
    "from prophet import Prophet\n",
    "from prophet.plot import add_changepoints_to_plot\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "from prophet.utilities import regressor_coefficients\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "from prophet.serialize import model_from_dict, model_to_dict, model_from_json, model_to_json, SIMPLE_ATTRIBUTES\n",
    "\n",
    "# Hyperopt:\n",
    "from hyperopt import fmin, tpe, hp, anneal, Trials, SparkTrials, STATUS_OK, space_eval\n",
    "\n",
    "# MLflow:\n",
    "import mlflow\n",
    "import mlflow.prophet\n",
    "from mlflow.models.signature import infer_signature\n",
    "mlflow_path = f\"file://{os.path.join(utils.root_path, 'mlruns')}\"\n",
    "\n",
    "\n",
    "\n",
    "# Definitions:\n",
    "granularity = \"Monthly\"\n",
    "category = \"Non-domestic\"\n",
    "\n",
    "# Do not show Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress log messages from cmdstanpy and prophet\n",
    "import logging\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.ERROR)\n",
    "logging.getLogger('prophet').setLevel(logging.ERROR)\n",
    "\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Data Pre-processing:\n",
    "# - Demand.\n",
    "# - Regressors: GDP, COVID Abnormalities, Yearly Seasonality, Monthly Weekdays\n",
    "# - Extra regressors that could be added:\n",
    "#   - Hours of daylight a month.\n",
    "#   - Number of weekends for Domestic Data.\n",
    "#   - etc.\n",
    "df = utils.make_complete_input_df(granularity=granularity[0], category=category)\n",
    "\n",
    "### Creating future DataFrame for forecasting with the utilised Regressors, Seasonality, etc.\n",
    "future_df = utils.make_forecast_df(granularity=granularity[0], category=category)\n",
    "\n",
    "\n",
    "\n",
    "## Modelling\n",
    "### Dummy Model Initialisation for easier Hyper-parameters' optimisation spaces\n",
    "model = utils.default_prophet_model(category=category, granularity=granularity[0])\n",
    "for reg in df.drop(columns=['ds', 'sector', 'y']).columns:\n",
    "  model.add_regressor(reg,\n",
    "                      mode='multiplicative')\n",
    "\n",
    "### HyperOpt Search Distributions, Search Spaces, Search Algorithm, etc.\n",
    "# HyperOpt:\n",
    "monthly_base_tuning = utils.monthly_base_tuning\n",
    "monthly_base_hyperopt = utils.monthly_base_hyperopt\n",
    "monthly_non_dom_regressors = utils.monthly_non_dom_regressors\n",
    "\n",
    "# Creating the HyperOpt Space for the tuning [We could use log instead of uniform?]\n",
    "space, space_dict = utils.create_hyperopt_space(model)\n",
    "\n",
    "# HyperOpt algorithm and +\n",
    "algo = tpe.suggest\n",
    "max_evals = 1\n",
    "trials = Trials()\n",
    "rstate = np.random.default_rng(42)\n",
    "random_state = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOpt's Prophet Optimization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for HyperOpt to run.\n",
    "# - The function is literally a model fitting with the HyperOpt variables and scoring depending on the cross_validation's metric (MAPE here)\n",
    "\n",
    "def hyperopt_tuning(hyperopt):\n",
    "\n",
    "  global model\n",
    "\n",
    "  # Creating the forecasting model with the variables we are using: multiplicative, holidays, regressors, seasonalities.\n",
    "  model = utils.default_prophet_model(category=category, granularity=granularity[0])\n",
    "\n",
    "  for reg in df.drop(columns=['ds', 'sector', 'y']).columns:\n",
    "    model.add_regressor(reg,\n",
    "                        mode='multiplicative')\n",
    "\n",
    "  # Using Hyperopt to tune the base hyperparams: changepoint_prior_scale, seasonality_prior_scale, holidays_prior_scale, changepoint_range.\n",
    "  for hyperparam in monthly_base_tuning:\n",
    "    setattr(model, hyperparam, hyperopt[hyperparam])\n",
    "\n",
    "  # Using Hyperopt to tune the extra hyperparams: regressors unique to the model.\n",
    "  for regressor_hyperparam in [i for i in model.extra_regressors]:\n",
    "    model.extra_regressors[regressor_hyperparam]['prior_scale'] = hyperopt[regressor_hyperparam]\n",
    "\n",
    "  # Fitting the model with the hyperopt variables to optimize them using the aglorithm \"Tree of Parzen Estimators\" by default. This can be changed.\n",
    "  model.fit(df)\n",
    "\n",
    "  # Scoring parameters for each iteration in the optimization.\n",
    "  df_cv, df_p, score = utils.scoring_outputs(model)\n",
    "\n",
    "  return {\n",
    "  'loss' : score,\n",
    "  'status' : STATUS_OK,\n",
    "  'cutoff_points' : df_p['cutoff_points'][0], \n",
    "  'horizon_days' : df_p['horizon'][0].days,\n",
    "  'metrics' : dict(zip(df_p.columns[1:-2], df_p.values[0][1:-2])),\n",
    "  'prophet_dict' : model_to_dict(model),\n",
    "  'category' : category,\n",
    "  'granularity' : granularity,\n",
    "  'hopt_algorithm' : algo.__module__,\n",
    "  'max_iters' : max_evals,\n",
    "  'random_state' : random_state,\n",
    "  'training_datetime' : datetime.datetime.today()\n",
    "  } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Optimization and tracking with mlflow\n",
    "- In this case the Loss Score is very high because of the artificial seasonalities placed because of COVID.\n",
    "- The model, at a cut_off point \"2021\" can't fit the extra seasonalities that are applied to the model.\n",
    "- Still, the model is getting the posterior distributions by fitting the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/11 20:32:14 WARNING mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics because creating `GPUMonitor` failed with error: `pynvml` is not installed, to log GPU metrics please run `pip install pynvml` to install it..\n",
      "2024/10/11 20:32:14 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4fa80f71614379b16578ca0051768b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.65s/trial, best loss: 0.055307066322500084]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d7d75212d8496d81c88fabd38af4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/11 20:32:18 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/10/11 20:32:18 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "# Experiments, Runs, Model values\n",
    "mlflow.set_tracking_uri(mlflow_path)\n",
    "exp_name = f\"GB_{granularity}\"\n",
    "experiment = mlflow.set_experiment(exp_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "run_name = str(datetime.datetime.today())[:19].replace(' ','_') + '_' + category.lower().replace('-', '_') + '_' + granularity.lower()\n",
    "model_name = 'model' + '_' + category.lower().replace('-', '_') + '_' + granularity.lower()\n",
    "\n",
    "# Datasets\n",
    "category_short_string = f\"{category}\".lower().replace(\"estic\",\"\").replace(\"-\",\"_\")\n",
    "dataset_1 = mlflow.data.from_pandas(df, source=utils.demand_path, name=f\"{category_short_string}_{granularity.lower()}_train_dataset\", targets=\"y\")\n",
    "dataset_2 = mlflow.data.from_pandas(future_df, source=utils.demand_path, name=f\"{category_short_string}_{granularity.lower()}_test_dataset\")\n",
    "if model.holidays is not None:\n",
    "  dataset_3 = mlflow.data.from_pandas(model.holidays, name=\"holidays\")\n",
    "\n",
    "mlflow.enable_system_metrics_logging()\n",
    "\n",
    "# Running the HyperOpt Function with mlflow as a context manager:\n",
    "with mlflow.start_run(run_name=run_name, experiment_id = experiment_id, log_system_metrics=True):\n",
    "\n",
    "  # In case we are wondering the paths for mlflow\n",
    "  tracking_uri = mlflow.get_tracking_uri() # for the log_metric, log_param\n",
    "  artifact_uri = mlflow.get_artifact_uri() # for the log_artifact\n",
    "  run_id = mlflow.active_run().info.run_id # the run_id\n",
    "\n",
    "  # Hyperparameter Optimisation: Getting the \"best\" prior distributions\n",
    "  hyperopt_optimization = fmin(fn=hyperopt_tuning,\n",
    "                                space = space,\n",
    "                                algo= algo,\n",
    "                                max_evals=max_evals,\n",
    "                                trials= trials,\n",
    "                                rstate= rstate)\n",
    "\n",
    "  # Retrieving the best best_model from the optimization by hyperopt + forecast.\n",
    "  best_model = model_from_dict(trials.best_trial['result']['prophet_dict'])\n",
    "  best_model.granularity = granularity[0]\n",
    "  best_model.forecast = best_model.predict(future_df)\n",
    "  best_model.full_forecast = utils.full_forecast_df(best_model, future=future_df, forecast=best_model.forecast)\n",
    "  best_model.reduced_forecast = utils.reduced_forecast_df(best_model, best_model.full_forecast)\n",
    "\n",
    "\n",
    "\n",
    "  ### MLflow:\n",
    "\n",
    "  ## MLflow: Basic definition: (In/Out)\n",
    "  # These are the steps to save the needed objects from the training.\n",
    "  # Signature (in/out schema):\n",
    "  signature = infer_signature(df, best_model.reduced_forecast)\n",
    "\n",
    "\n",
    "  ## MLflow: Getting variables to save, archive:\n",
    "  # Input DataFrame:\n",
    "  df.to_parquet('input_df.parquet')\n",
    "  future_df.to_parquet('future_df.parquet')\n",
    "\n",
    "  # Input holidays if any:\n",
    "  if best_model.holidays is not None:\n",
    "    best_model.holidays.to_parquet('input_holidays.parquet')\n",
    "\n",
    "  # Output DataFrame:\n",
    "  best_model.full_forecast.to_parquet('forecast_full_df.parquet')\n",
    "  best_model.reduced_forecast.to_parquet('forecast_reduced_df.parquet')\n",
    "\n",
    "  # Getting the serialized best_model:\n",
    "  with open('prophet_model.json', 'w') as json_file:\n",
    "      json_file.write(model_to_json(best_model))\n",
    "\n",
    "  # Parameters of the best best_model:\n",
    "  with open('prophet_model_parameters.pkl', 'wb') as file:\n",
    "      pickle.dump(best_model.params, file)\n",
    "\n",
    "  # Coefficients and values of the extra_regressors:\n",
    "  regressor_coefficients(best_model).to_parquet('regressors_coefficients.parquet')\n",
    "\n",
    "  # Best best_model's extra_regressors:\n",
    "  with open('prophet_model_extra_regressors.pkl', 'wb') as file:\n",
    "      pickle.dump(best_model.extra_regressors, file)\n",
    "  \n",
    "  # Best best_model's fourier series seasonalities:\n",
    "  if len(best_model.seasonalities) > 0:\n",
    "    with open('prophet_model_fourier_seasonalities.pkl', 'wb') as file:\n",
    "        pickle.dump(best_model.seasonalities, file)\n",
    "\n",
    "  # Best best_model's Pre-processed (by Prophet) inputs:\n",
    "  preprocess = best_model.preprocess(df)\n",
    "  with open('prophet_model_preprocessed_params.pkl', 'wb') as file:\n",
    "      pickle.dump(preprocess, file)\n",
    "\n",
    "  # Initial hyperparams:\n",
    "  with open('prophet_model_initial_hyperparams.pkl', 'wb') as file:\n",
    "      pickle.dump(best_model.calculate_initial_params(num_total_regressors=preprocess.K), file)\n",
    "\n",
    "  # The main inputs into the Prophet object:\n",
    "  params_prophet = {}\n",
    "  for i in [j for j in SIMPLE_ATTRIBUTES if j not in ['component_modes', 'country_holidays', 'logistic_floor']]:\n",
    "    params_prophet[i] = (getattr(best_model,i))\n",
    "\n",
    "  # Saving HyperOpt outputs:\n",
    "  hopt = utils.hyper_params_df(trials).to_parquet(\"hyperopt_run.parquet\")\n",
    "\n",
    "  # Saving graphs:\n",
    "  utils.plot_forecast_changepoints(best_model).savefig(\"forecast_changepoints.png\")\n",
    "  plt.close()\n",
    "  utils.plot_base_components(best_model).savefig('base_components.png')\n",
    "  plt.close()\n",
    "  utils.plot_noise_ts(best_model).savefig(\"errors_timeseries.png\")\n",
    "  plt.close()\n",
    "  utils.plot_error_hist(best_model).savefig(\"errors_histogram.png\")\n",
    "  plt.close()\n",
    "  utils.plot_inputs_in_time(best_model).savefig(\"variables_in_time.png\")\n",
    "  plt.close()\n",
    "  utils.plot_regressors(best_model).savefig(\"regressors_in_time.png\")\n",
    "  plt.close()\n",
    "  utils.plot_regressors_linearity(best_model).savefig(\"regressors_impact_on_trend.png\")\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "  ## MLflow: Logging variables (Parameters, Metrics, Artifacts):\n",
    "  # Inputs (datasets):\n",
    "  mlflow.log_input(dataset_1, context=\"training\")\n",
    "  mlflow.log_input(dataset_2, context=\"testing\")\n",
    "  if best_model.holidays is not None:\n",
    "    mlflow.log_input(dataset_3, context=\"holidays\")\n",
    "\n",
    "  # Paramters used when running the model:\n",
    "  mlflow.log_param(\"horizon_days\", trials.best_trial['result']['horizon_days'])\n",
    "  mlflow.log_param(\"cutoff_points\", trials.best_trial['result']['cutoff_points'])\n",
    "  mlflow.log_param(\"hopt_algorithm\", algo.__module__)\n",
    "  mlflow.log_param(\"hopt_space\", space_dict)\n",
    "\n",
    "  for key, value in params_prophet.items():\n",
    "    mlflow.log_param(key, value)\n",
    "\n",
    "  if len(best_model.seasonalities) > 0:\n",
    "    mlflow.log_param(\"seasonalities\", best_model.seasonalities)\n",
    "  mlflow.log_param(\"regressors\", best_model.extra_regressors)\n",
    "\n",
    "  # Logging metrics: these are numeric variables depicting scores of the model.\n",
    "  for metric, value in trials.best_trial['result']['metrics'].items():\n",
    "    mlflow.log_metric(\"cross_validation_{}\".format(metric), value)\n",
    "  mlflow.log_metric(\"cross_validation_mape\", trials.best_trial['result']['loss'])\n",
    "  mlflow.log_metric(\"hopt_max_iters\", len(trials.trials))\n",
    "  mlflow.log_metric(\"hopt_random_state\", random_state)\n",
    "\n",
    "  # Logging artifacts: these are files like json, tables, pictures, etc.\n",
    "  mlflow.log_artifact(local_path = \"input_df.parquet\", artifact_path =\"datasets\")\n",
    "  mlflow.log_artifact(local_path = \"future_df.parquet\", artifact_path =\"datasets\")\n",
    "  if best_model.holidays is not None:\n",
    "    mlflow.log_artifact(local_path = \"input_holidays.parquet\", artifact_path = \"datasets\")\n",
    "  mlflow.log_artifact(local_path = \"prophet_model.json\", artifact_path = \"main_model_outputs\")\n",
    "  mlflow.log_artifact(local_path = \"prophet_model_parameters.pkl\", artifact_path = \"main_model_outputs\")\n",
    "  mlflow.log_artifact(local_path = \"forecast_full_df.parquet\", artifact_path = \"forecast_outputs\")\n",
    "  mlflow.log_artifact(local_path = \"forecast_reduced_df.parquet\", artifact_path = \"forecast_outputs\")\n",
    "  mlflow.log_artifact(local_path = \"prophet_model_extra_regressors.pkl\", artifact_path = \"added_model_outputs\")\n",
    "  mlflow.log_artifact(local_path = \"regressors_coefficients.parquet\", artifact_path = \"added_model_outputs\")\n",
    "  if len(best_model.seasonalities) > 0:\n",
    "    mlflow.log_artifact(local_path = \"prophet_model_fourier_seasonalities.pkl\", artifact_path = \"added_model_outputs\")\n",
    "  mlflow.log_artifact(local_path = \"prophet_model_preprocessed_params.pkl\", artifact_path = \"interim_model_outputs\")\n",
    "  mlflow.log_artifact(local_path = \"prophet_model_initial_hyperparams.pkl\", artifact_path = \"interim_model_outputs\")\n",
    "  mlflow.log_artifact(local_path = \"hyperopt_run.parquet\", artifact_path = \"hyperopt\")\n",
    "  for fig in [graph for graph in os.listdir() if '.png' in graph]:\n",
    "    mlflow.log_artifact(local_path = fig, artifact_path = \"graphs\")\n",
    "\n",
    "\n",
    "  ## MLflow: Getting the requirements.txt for logging the model\n",
    "  # Get all installed package names and versions from pkg_resources.working_set\n",
    "  installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "  # Filter only modules that are actually installed packages and get their versions\n",
    "  imported_modules = {}\n",
    "  for module_name in sys.modules:\n",
    "      # Check if the module is an installed package\n",
    "      if module_name in installed_packages:\n",
    "          version = installed_packages[module_name]\n",
    "          imported_modules[module_name] = version\n",
    "  # Convert to DataFrame for easy reading or export\n",
    "  modules_df = pd.DataFrame(list(imported_modules.items()), columns=[\"Package\", \"Version\"])\n",
    "  modules_df['formatted'] = modules_df[\"Package\"] + \"==\" + modules_df[\"Version\"]\n",
    "  modules_df['formatted'].to_csv(\"requirements.txt\", index=False, header=False)\n",
    "  \n",
    "  mlflow.prophet.log_model(best_model,\n",
    "                           artifact_path=model_name,\n",
    "                           signature=signature,\n",
    "                           input_example=df,\n",
    "                           pip_requirements=\"requirements.txt\")\n",
    "\n",
    "  # We do not need the local files > so we'll delete them as they should be temp.\n",
    "  for temp in os.listdir():\n",
    "    if ('.parquet' in temp) or ('.json' in temp) or ('.png' in temp) or ('.txt' in temp) or ('.pkl' in temp):\n",
    "      os.remove(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.reduced_forecast.to_parquet(os.path.join(utils.outputs_folder, 'monthly', f\"{category}_{granularity}_reduced_forecast.parquet\"))\n",
    "best_model.full_forecast.to_parquet(os.path.join(utils.outputs_folder, 'monthly', f\"{category}_{granularity}_full_forecast.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pablo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
